{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9837777",
   "metadata": {},
   "source": [
    "# Classification of fake news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca13f8f",
   "metadata": {},
   "source": [
    "I am using the dataset from Kaggle. I first want to retrieve this and convert it to a dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc27305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c086d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer det første datasæt\n",
    "df1 = pd.read_csv(\"True.csv\")\n",
    "df2 = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "X = list(df[\"text\"])\n",
    "\n",
    "# Lav target array, hvor 0 er en troværdig artikel og 1 er en utroværdig artikel\n",
    "y1 = np.zeros(len(df1))\n",
    "y2 = np.ones(len(df2))\n",
    "y = np.hstack((y1, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cda6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer det andet datasæt\n",
    "df_second = pd.read_csv(\"Train.csv\")\n",
    "X2 = list(df_second[\"text\"])\n",
    "\n",
    "y_2 = np.array(df_second[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e79f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forbered første datasæt ved at fjerne specialkarakterer, stopwords og stemme de resterende ord\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = []\n",
    "\n",
    "# Iterer over artiklerne og fjern specialtegn\n",
    "for text in X:\n",
    "    text_oa = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    text_oa = text_oa.lower()\n",
    "    text_oa = text_oa.split()\n",
    "\n",
    "    # stopwords filtreres fra, og ordene stemmes\n",
    "\n",
    "    filtered_article = [ps.stem(w) for w in text_oa if w not in stop_words and len(w) > 1]\n",
    "    corpus.append(\" \".join(filtered_article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1a11f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed at index 142\n",
      "Failed at index 573\n",
      "Failed at index 1200\n",
      "Failed at index 1911\n",
      "Failed at index 2148\n",
      "Failed at index 2169\n",
      "Failed at index 2793\n",
      "Failed at index 3329\n",
      "Failed at index 3729\n",
      "Failed at index 4288\n",
      "Failed at index 4358\n",
      "Failed at index 5717\n",
      "Failed at index 6215\n",
      "Failed at index 6680\n",
      "Failed at index 8649\n",
      "Failed at index 8908\n",
      "Failed at index 8922\n",
      "Failed at index 9350\n",
      "Failed at index 9446\n",
      "Failed at index 9454\n",
      "Failed at index 9524\n",
      "Failed at index 10466\n",
      "Failed at index 10867\n",
      "Failed at index 11450\n",
      "Failed at index 11486\n",
      "Failed at index 12056\n",
      "Failed at index 12460\n",
      "Failed at index 12835\n",
      "Failed at index 13020\n",
      "Failed at index 13107\n",
      "Failed at index 13915\n",
      "Failed at index 14499\n",
      "Failed at index 14933\n",
      "Failed at index 16126\n",
      "Failed at index 18479\n",
      "Failed at index 18757\n",
      "Failed at index 19157\n",
      "Failed at index 19227\n",
      "Failed at index 19388\n"
     ]
    }
   ],
   "source": [
    "# Manipulering af det andet datasæt. Specialkarakterer og stopwords fjernes og der stemmes\n",
    "corpus2 = []\n",
    "failed_files = []\n",
    "# Iterer over artiklerne og fjern specialtegn\n",
    "for i, text in enumerate(X2):\n",
    "    # Nogen af artiklerne i dette datasæt er corrupted, så de fjernes med denne try-blok\n",
    "    try:\n",
    "        text_oa = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "        text_oa = text_oa.lower()\n",
    "        text_oa = text_oa.split()\n",
    "\n",
    "        # stopwords filtreres fra, og ordene stemmes\n",
    "\n",
    "        filtered_article = [ps.stem(w) for w in text_oa if w not in stop_words and len(w) > 1]\n",
    "        corpus2.append(\" \".join(filtered_article))\n",
    "    except TypeError:\n",
    "        failed_files.append(i)\n",
    "        \n",
    "# Tilpas target for andet datasæt sådan så der tages højde for manglende artikler:\n",
    "y_2 = np.delete(y_2, failed_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aedb024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "#Undersøg artiklerne efter behandlingen\n",
    "print(X2[142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c121517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 89633\n",
      "bag_of_words: <44898x89633 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 6792176 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Corpus for første datasæt omdannes til en bag_of_words repræsentation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "\n",
    "bag_of_words = vect.transform(corpus)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f3b31e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 110429\n",
      "bag_of_words: <20761x89633 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 4953868 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Corpus for andet datasæt omdannes ligeledes til en bag_of_words\n",
    "vect2 = CountVectorizer()\n",
    "vect2.fit(corpus2)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect2.vocabulary_)))\n",
    "\n",
    "bag_of_words2 = vect.transform(corpus2)\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39cfdfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 89633\n",
      "Every 500th feature:\n",
      "['aa', 'accorhotel', 'aerosmith', 'airi', 'alhorir', 'ambassadori', 'angelu', 'apollonian', 'armsseattl', 'atacama', 'aw', 'baharim', 'bariloch', 'bcuzimdamomma', 'benghalem', 'bia', 'blackist', 'bobic', 'botox', 'breitbartnoah', 'brusselsattack', 'bushmast', 'called', 'carlisl', 'cbtxrrf', 'chakrabarti', 'chewbacca', 'chunki', 'clevezirm', 'cointelpro', 'concur', 'corettascottk', 'craigslist', 'cubano', 'dahman', 'davidpodhaski', 'defil', 'derfbackderf', 'dietl', 'disturb', 'donerson', 'dribbl', 'duz', 'edg', 'elena', 'endyszf', 'error', 'evgeni', 'facetim', 'fauzia', 'fflvx', 'flagupon', 'forbesa', 'freeh', 'fuzzi', 'garthwait', 'geqoqiefeatur', 'giordi', 'goic', 'graf', 'grumpi', 'gvcqltko', 'hambantota', 'haskel', 'hehellaasian', 'hgxwaopmtz', 'hlq', 'horsepox', 'humza', 'icrl', 'imelda', 'inexplic', 'interplay', 'islami', 'jaffna', 'jcr', 'jiminez', 'josearielcueva', 'jvt', 'karamagi', 'keeper', 'khatm', 'kitson', 'konta', 'ktvu', 'lafeer', 'laundri', 'leighdow', 'libyaliberti', 'lizabethmcnabb', 'lothar', 'luster', 'maggiepriceless', 'manal', 'markand', 'matthaig', 'mcinnerney', 'melbourn', 'mgmz', 'mina', 'mkru', 'montanamo', 'mplvlqxgdi', 'mundlo', 'nadav', 'navarett', 'netanyayu', 'niang', 'noblest', 'novick', 'nyp', 'offsid', 'ongili', 'osdim', 'overwhelmingli', 'pantaleo', 'paulbegala', 'penzey', 'philander', 'piza', 'policynow', 'potomac', 'preval', 'pruchni', 'pw', 'qq', 'radic', 'rastafarian', 'recklessli', 'releasethetap', 'retch', 'rightwingangel', 'rodger', 'rpbp', 'rvqtfikgbu', 'salehi', 'saro', 'schlong', 'seanspic', 'serdjiu', 'shaqir', 'shoebat', 'silj', 'skvern', 'snif', 'sorkin', 'sporck', 'statesread', 'stopthenutjob', 'successor', 'sushchin', 'szwzcgxxz', 'tarkanian', 'teller', 'theatr', 'throatedli', 'tjex', 'torment', 'tremain', 'trumpswal', 'twcintg', 'udim', 'unconvent', 'unnecessari', 'uqwlodkw', 'valentinorockstudheel', 'versabut', 'vitti', 'vwlexdhkoom', 'wassenaar', 'wembley', 'wich', 'wjc', 'wray', 'xgmcki', 'yamaguchi', 'ylruk', 'yzsbiesbnb', 'zhanjiang', 'zunvjl']\n",
      "Number of features: 110429\n",
      "Every 500th feature:\n",
      "['aa', 'abugraib', 'adamweinstein', 'africana', 'airweav', 'alesmith', 'alright', 'amorph', 'ani', 'anular', 'arabica', 'arrepentido', 'assfest', 'aufgefasst', 'avineri', 'badawi', 'bangkok', 'baskervil', 'bedauern', 'bencic', 'besra', 'bilater', 'blackliberationcollect', 'bnv', 'borax', 'brainiac', 'britch', 'bugsi', 'butuh', 'calvez', 'carcer', 'catecholamin', 'cerebellum', 'chatbot', 'chinesischen', 'cicada', 'clawless', 'cocula', 'commentait', 'conforto', 'contrariwis', 'cosden', 'crave', 'cryptocurr', 'cyberbulli', 'dangereux', 'deadli', 'deitsch', 'depar', 'destitu', 'didnt', 'disconnected', 'dmca', 'dormant', 'drohn', 'dusit', 'ecut', 'einwandern', 'elvina', 'enfermedad', 'epitaph', 'esea', 'eurangloland', 'expandierten', 'faintheart', 'fauna', 'fetchingli', 'firest', 'fluorideg', 'forthrightli', 'fremdel', 'fune', 'gane', 'gebiet', 'gepubliceerd', 'gibbon', 'gli', 'gonorrhoea', 'grathwohl', 'growth', 'gurney', 'hakimiyet', 'haraway', 'hayrid', 'hellstorm', 'hia', 'hjelt', 'honolulu', 'huangpu', 'hyperaggress', 'ignoran', 'importantli', 'inerek', 'inspit', 'inversora', 'islington', 'jahbat', 'jedno', 'jiuquan', 'jubileumgrati', 'kail', 'karpiouk', 'kelti', 'kiahyzd', 'klang', 'komplexen', 'kress', 'kuzzo', 'lancet', 'lavasani', 'leist', 'lib', 'linol', 'lockheedmartin', 'lowey', 'lyasoff', 'magnetit', 'malvasia', 'marcio', 'maslenjak', 'mba', 'mechkina', 'menchu', 'metuchen', 'miklo', 'misaim', 'mochi', 'monroevil', 'motieven', 'mujer', 'mwai', 'nanomachin', 'ncesi', 'neta', 'nickcunningham', 'noam', 'norkin', 'nukeprofession', 'obtus', 'ojito', 'oosthuizen', 'orthographisch', 'overbook', 'pah', 'paramed', 'patisseri', 'pelvis', 'perth', 'phytochem', 'pitof', 'pod', 'popsi', 'pptv', 'pressley', 'programmausf', 'psoe', 'pwxwyq', 'quiebr', 'raisman', 'rawer', 'recondit', 'regressivist', 'repair', 'retiro', 'ridership', 'rnt', 'rorschach', 'rubu', 'sabha', 'saltma', 'sashay', 'scheffler', 'scoliosi', 'seferberlikl', 'septum', 'shaki', 'shibboleth', 'shunko', 'simopoulo', 'skosh', 'smukler', 'solu', 'spade', 'sppjfu', 'stapl', 'stie', 'striven', 'suffici', 'superstor', 'swithin', 'takagi', 'tasnim', 'telescreen', 'textualment', 'thirdli', 'timespan', 'tomblin', 'tracker', 'treuer', 'truthdig', 'tuviera', 'ukrainsk', 'undefens', 'unison', 'untim', 'utilizava', 'vasek', 'verh', 'vieja', 'vladdo', 'vulgarian', 'warm', 'weimerskirch', 'whini', 'windmil', 'wonderkid', 'wyda', 'yapmadan', 'yorgi', 'zare', 'ziv']\n"
     ]
    }
   ],
   "source": [
    "# Undersøg parametre for bag_of_words\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"Every 500th feature:\\n{}\".format(feature_names[::500]))\n",
    "\n",
    "feature_names2 = vect2.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names2)))\n",
    "print(\"Every 500th feature:\\n{}\".format(feature_names2[::500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fb7dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training articles: 33673\n",
      "Test articles: 11225\n",
      "Second dataset\n",
      "Training articles: 15570\n",
      "Test articles: 5191\n"
     ]
    }
   ],
   "source": [
    "# Dataen opdeles nu i en træningsdel og en testdel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datasæt 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, y, random_state=0)\n",
    "print(\"Training articles: {}\\nTest articles: {}\".format(np.size(y_train), np.size(y_test)))\n",
    "\n",
    "# Datasæt 2\n",
    "# Jeg gør det samme for datasæt 2, sådan så  jeg får 2 modeller\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(bag_of_words2, y_2, random_state=0)\n",
    "print(\"Second dataset\\nTraining articles: {}\\nTest articles: {}\".format(np.size(y_train2), np.size(y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "994205d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 training test score: 0.9999703026163395\n",
      "Dataset 1 test score: 0.9951002227171493\n",
      "Dataset 2 test score: 0.5520446991956072\n",
      "Second model dataset 1 test score: 0.6953229398663697\n",
      "First model dataset 2 test score: 0.9504912348295126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "print(\"Dataset 1 training test score: {}\".format(log_reg.score(X_train, y_train)))\n",
    "print(\"Dataset 1 test score: {}\".format(log_reg.score(X_test, y_test)))\n",
    "print(\"Dataset 2 test score: {}\".format(log_reg.score(bag_of_words2, y_2)))\n",
    "\n",
    "# Model trænet på det andet datasæt\n",
    "log_reg2 = LogisticRegression(max_iter=1000).fit(X_train2, y_train2)\n",
    "print(\"Second model dataset 1 test score: {}\".format(log_reg2.score(X_test, y_test)))\n",
    "print(\"First model dataset 2 test score: {}\".format(log_reg2.score(X_test2, y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ad64da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score when trained on first dataset: 0.9943278014547502\n",
      "Average cross-validation score when trained on second dataset: 0.9481719113837898\n"
     ]
    }
   ],
   "source": [
    "# Jeg tester modellens generaliseringspotentiale med cross-validation på begge datasæt.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "first_scores = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "second_scores = cross_val_score(log_reg, bag_of_words2, y_2, cv=5)\n",
    "print(\"Average cross-validation score when trained on first dataset: {}\".format(first_scores.mean()))\n",
    "print(\"Average cross-validation score when trained on second dataset: {}\".format(second_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7289033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce540b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
