{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c2aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c7fff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer det første datasæt\n",
    "df1 = pd.read_csv(\"True.csv\")\n",
    "df2 = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "X1 = list(df[\"text\"])\n",
    "\n",
    "# Lav target array, hvor 0 er en troværdig artikel og 1 er en utroværdig artikel\n",
    "y1 = np.zeros(len(df1))\n",
    "y2 = np.ones(len(df2))\n",
    "y1 = np.hstack((y1, y2))\n",
    "\n",
    "# Importer det andet datasæt\n",
    "df_second = pd.read_csv(\"Train.csv\")\n",
    "X2 = list(df_second[\"text\"])\n",
    "\n",
    "y2 = np.array(df_second[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df2446c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399852 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Nu importerer jeg word_embeddingen\n",
    "# Det meste af denne kode er taget fra https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/\n",
    "# Jeg ønsker ikke at undersøge stopwords, så disse skal fjernes fra embeddings_index\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('stanford_embedding/glove.6B.100d.txt', 'r', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    if word not in stop_words:\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "75341bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for word, v in embeddings_index.items():\n",
    "    if not isinstance(v, np.ndarray):\n",
    "        print(\"{} has an enmpty value\".format(word))\n",
    "        count += 1\n",
    "        \n",
    "if count == 0:\n",
    "    print(\"No corrupted \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4b3e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800,)\n",
      "(20761,)\n",
      "['washington', 'reuters', 'head', 'conservative', 'republican', 'faction', 'congress', 'voted', 'month', 'huge', 'expansion', 'national', 'debt', 'pay', 'tax', 'cuts', 'called', 'fiscal', 'conservative', 'sunday', 'urged', 'budget', 'restraint', 'keeping', 'sharp', 'pivot', 'way', 'among', 'republicans', 'representative', 'mark', 'meadows', 'speaking', 'cbs', 'face', 'nation', 'drew', 'hard', 'line', 'federal', 'spending', 'lawmakers', 'bracing', 'battle', 'january', 'return', 'holidays', 'wednesday', 'lawmakers', 'begin', 'trying', 'pass', 'federal', 'budget', 'fight', 'likely', 'linked', 'issues', 'immigration', 'policy', 'even', 'november', 'congressional', 'election', 'campaigns', 'approach', 'republicans', 'seek', 'keep', 'control', 'congress', 'president', 'donald', 'trump', 'republicans', 'want', 'big', 'budget', 'increase', 'military', 'spending', 'democrats', 'also', 'want', 'proportional', 'increases', 'non', 'defense', 'discretionary', 'spending', 'programs', 'support', 'education', 'scientific', 'research', 'infrastructure', 'public', 'health', 'environmental', 'protection', 'trump', 'administration', 'already', 'willing', 'say', 'going', 'increase', 'non', 'defense', 'discretionary', 'spending', 'percent', 'meadows', 'chairman', 'small', 'influential', 'house', 'freedom', 'caucus', 'said', 'program', 'democrats', 'saying', 'enough', 'need', 'give', 'government', 'pay', 'raise', 'percent', 'fiscal', 'conservative', 'see', 'rationale', 'eventually', 'run', 'people', 'money', 'said', 'meadows', 'among', 'republicans', 'voted', 'late', 'december', 'party', 'debt', 'financed', 'tax', 'overhaul', 'expected', 'balloon', 'federal', 'budget', 'deficit', 'add', 'trillion', 'years', 'trillion', 'national', 'debt', 'interesting', 'hear', 'mark', 'talk', 'fiscal', 'responsibility', 'democratic', 'representative', 'joseph', 'crowley', 'said', 'cbs', 'crowley', 'said', 'republican', 'tax', 'bill', 'would', 'require', 'united', 'states', 'borrow', 'trillion', 'paid', 'future', 'generations', 'finance', 'tax', 'cuts', 'corporations', 'rich', 'one', 'least', 'fiscally', 'responsible', 'bills', 'ever', 'seen', 'passed', 'history', 'house', 'representatives', 'think', 'going', 'paying', 'many', 'many', 'years', 'come', 'crowley', 'said', 'republicans', 'insist', 'tax', 'package', 'biggest', 'tax', 'overhaul', 'years', 'boost', 'economy', 'job', 'growth', 'house', 'speaker', 'paul', 'ryan', 'also', 'supported', 'tax', 'bill', 'recently', 'went', 'meadows', 'making', 'clear', 'radio', 'interview', 'welfare', 'entitlement', 'reform', 'party', 'often', 'calls', 'would', 'top', 'republican', 'priority', 'republican', 'parlance', 'entitlement', 'programs', 'mean', 'food', 'stamps', 'housing', 'assistance', 'medicare', 'medicaid', 'health', 'insurance', 'elderly', 'poor', 'disabled', 'well', 'programs', 'created', 'washington', 'assist', 'needy', 'democrats', 'seized', 'ryan', 'early', 'december', 'remarks', 'saying', 'showed', 'republicans', 'would', 'try', 'pay', 'tax', 'overhaul', 'seeking', 'spending', 'cuts', 'social', 'programs', 'goals', 'house', 'republicans', 'may', 'take', 'back', 'seat', 'senate', 'votes', 'democrats', 'needed', 'approve', 'budget', 'prevent', 'government', 'shutdown', 'democrats', 'use', 'leverage', 'senate', 'republicans', 'narrowly', 'control', 'defend', 'discretionary', 'non', 'defense', 'programs', 'social', 'spending', 'tackling', 'issue', 'dreamers', 'people', 'brought', 'illegally', 'country', 'children', 'trump', 'september', 'put', 'march', 'expiration', 'date', 'deferred', 'action', 'childhood', 'arrivals', 'daca', 'program', 'protects', 'young', 'immigrants', 'deportation', 'provides', 'work', 'permits', 'president', 'said', 'recent', 'twitter', 'messages', 'wants', 'funding', 'proposed', 'mexican', 'border', 'wall', 'immigration', 'law', 'changes', 'exchange', 'agreeing', 'help', 'dreamers', 'representative', 'debbie', 'dingell', 'told', 'cbs', 'favor', 'linking', 'issue', 'policy', 'objectives', 'wall', 'funding', 'need', 'daca', 'clean', 'said', 'wednesday', 'trump', 'aides', 'meet', 'congressional', 'leaders', 'discuss', 'issues', 'followed', 'weekend', 'strategy', 'sessions', 'trump', 'republican', 'leaders', 'jan', 'white', 'house', 'said', 'trump', 'also', 'scheduled', 'meet', 'sunday', 'florida', 'republican', 'governor', 'rick', 'scott', 'wants', 'emergency', 'aid', 'house', 'passed', 'billion', 'aid', 'package', 'hurricanes', 'florida', 'texas', 'puerto', 'rico', 'wildfires', 'california', 'package', 'far', 'exceeded', 'billion', 'requested', 'trump', 'administration', 'senate', 'yet', 'voted', 'aid']\n",
      "['house', 'dem', 'aide', 'even', 'see', 'comey', 'letter', 'jason', 'chaffetz', 'tweeted', 'darrell', 'lucus', 'october', 'subscribe', 'jason', 'chaffetz', 'stump', 'american', 'fork', 'utah', 'image', 'courtesy', 'michael', 'jolley', 'available', 'creative', 'commons', 'license', 'apologies', 'keith', 'olbermann', 'doubt', 'worst', 'person', 'world', 'week', 'fbi', 'director', 'james', 'comey', 'according', 'house', 'democratic', 'aide', 'looks', 'like', 'also', 'know', 'second', 'worst', 'person', 'well', 'turns', 'comey', 'sent', 'infamous', 'letter', 'announcing', 'fbi', 'looking', 'emails', 'may', 'related', 'hillary', 'clinton', 'email', 'server', 'ranking', 'democrats', 'relevant', 'committees', 'hear', 'comey', 'found', 'via', 'tweet', 'one', 'republican', 'committee', 'chairmen', 'know', 'comey', 'notified', 'republican', 'chairmen', 'democratic', 'ranking', 'members', 'house', 'intelligence', 'judiciary', 'oversight', 'committees', 'agency', 'reviewing', 'emails', 'recently', 'discovered', 'order', 'see', 'contained', 'classified', 'information', 'long', 'letter', 'went', 'oversight', 'committee', 'chairman', 'jason', 'chaffetz', 'set', 'political', 'world', 'ablaze', 'tweet', 'fbi', 'dir', 'informed', 'fbi', 'learned', 'existence', 'emails', 'appear', 'pertinent', 'investigation', 'case', 'reopened', 'jason', 'chaffetz', 'jasoninthehouse', 'october', 'course', 'know', 'case', 'comey', 'actually', 'saying', 'reviewing', 'emails', 'light', 'unrelated', 'case', 'know', 'anthony', 'weiner', 'sexting', 'teenager', 'apparently', 'little', 'things', 'facts', 'matter', 'chaffetz', 'utah', 'republican', 'already', 'vowed', 'initiate', 'raft', 'investigations', 'hillary', 'wins', 'least', 'two', 'years', 'worth', 'possibly', 'entire', 'term', 'worth', 'apparently', 'chaffetz', 'thought', 'fbi', 'already', 'work', 'resulting', 'tweet', 'briefly', 'roiled', 'nation', 'cooler', 'heads', 'realized', 'dud', 'according', 'senior', 'house', 'democratic', 'aide', 'misreading', 'letter', 'may', 'least', 'chaffetz', 'sins', 'aide', 'told', 'shareblue', 'boss', 'democrats', 'even', 'know', 'comey', 'letter', 'time', 'found', 'checked', 'twitter', 'democratic', 'ranking', 'members', 'relevant', 'committees', 'receive', 'comey', 'letter', 'republican', 'chairmen', 'fact', 'democratic', 'ranking', 'members', 'receive', 'chairman', 'oversight', 'government', 'reform', 'committee', 'jason', 'chaffetz', 'tweeted', 'made', 'public', 'let', 'see', 'got', 'right', 'fbi', 'director', 'tells', 'chaffetz', 'gop', 'committee', 'chairmen', 'major', 'development', 'potentially', 'politically', 'explosive', 'investigation', 'neither', 'chaffetz', 'colleagues', 'courtesy', 'let', 'democratic', 'counterparts', 'know', 'instead', 'according', 'aide', 'made', 'find', 'twitter', 'already', 'talk', 'daily', 'kos', 'comey', 'provided', 'advance', 'notice', 'letter', 'chaffetz', 'republicans', 'giving', 'time', 'turn', 'spin', 'machine', 'may', 'make', 'good', 'theater', 'nothing', 'far', 'even', 'suggests', 'case', 'nothing', 'far', 'suggests', 'comey', 'anything', 'grossly', 'incompetent', 'tone', 'deaf', 'suggest', 'however', 'chaffetz', 'acting', 'way', 'makes', 'dan', 'burton', 'darrell', 'issa', 'look', 'like', 'models', 'responsibility', 'bipartisanship', 'even', 'decency', 'notify', 'ranking', 'member', 'elijah', 'cummings', 'something', 'explosive', 'trample', 'basic', 'standards', 'fairness', 'know', 'granted', 'likely', 'chaffetz', 'answer', 'sits', 'ridiculously', 'republican', 'district', 'anchored', 'provo', 'orem', 'cook', 'partisan', 'voting', 'index', 'gave', 'mitt', 'romney', 'punishing', 'percent', 'vote', 'moreover', 'republican', 'house', 'leadership', 'given', 'full', 'support', 'chaffetz', 'planned', 'fishing', 'expedition', 'mean', 'turn', 'hot', 'lights', 'textbook', 'example', 'house', 'become', 'republican', 'control', 'also', 'second', 'worst', 'person', 'world', 'darrell', 'lucus', 'darrell', 'something', 'graduate', 'university', 'north', 'carolina', 'considers', 'journalist', 'old', 'school', 'attempt', 'turn', 'member', 'religious', 'right', 'college', 'succeeded', 'turning', 'religious', 'right', 'worst', 'nightmare', 'charismatic', 'christian', 'unapologetic', 'liberal', 'desire', 'stand', 'scared', 'silence', 'increased', 'survived', 'abusive', 'three', 'year', 'marriage', 'may', 'know', 'daily', 'kos', 'christian', 'dem', 'nc', 'follow', 'twitter', 'darrelllucus', 'connect', 'facebook', 'click', 'buy', 'darrell', 'mello', 'yello', 'connect']\n"
     ]
    }
   ],
   "source": [
    "# Dataen laves om, så den ikke indeholder specialkarakterer eller stopwords\n",
    "# Datasæt 1\n",
    "# Iterer over artiklerne og fjern specialtegn\n",
    "import re\n",
    "corpus1 = []\n",
    "for text in X1:\n",
    "    text_oa = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    text_oa = text_oa.lower()\n",
    "    text_oa = text_oa.split()\n",
    "\n",
    "    # stopwords filtreres fra, og ordene stemmes\n",
    "\n",
    "    filtered_article = [w for w in text_oa if w not in stop_words and len(w) > 1]\n",
    "    corpus1.append(filtered_article)\n",
    "    \n",
    "# Datasæt 2\n",
    "# Iterer over artiklerne og fjern specialtegn\n",
    "corpus2 = []\n",
    "failed_files = []\n",
    "for i, text in enumerate(X2):\n",
    "    try:\n",
    "        text_oa = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "        text_oa = text_oa.lower()\n",
    "        text_oa = text_oa.split()\n",
    "\n",
    "        # stopwords filtreres fra, og ordene stemmes\n",
    "\n",
    "        filtered_article = [w for w in text_oa if w not in stop_words and len(w) > 1]\n",
    "        corpus2.append(filtered_article)\n",
    "    except TypeError:\n",
    "        failed_files.append(i)\n",
    "        \n",
    "# Tilpas target for andet datasæt sådan så der tages højde for manglende artikler:\n",
    "print(np.shape(y2))\n",
    "y2 = np.delete(y2, failed_files)\n",
    "print(np.shape(y2))\n",
    "\n",
    "        \n",
    "print(corpus1[0])\n",
    "print(corpus2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f626d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 100)\n",
      "(20761, 100)\n"
     ]
    }
   ],
   "source": [
    "# Opret vektor-repræsentationer af dokumenter som vægtede gennemsnit af word-embedding\n",
    "# Datasæt 1\n",
    "# Initialiser matricen, der skal holde artikel-vektorerne\n",
    "vector_length = 100\n",
    "art_matrix = np.zeros((len(corpus1), vector_length))\n",
    "print(np.shape(art_matrix))\n",
    "\n",
    "for i, text in enumerate(corpus1):\n",
    "        art_vect = np.mean([np.array(embeddings_index[w]) for w in text if w in embeddings_index.keys()],\n",
    "                           axis=0)\n",
    "        art_matrix[i] = art_vect\n",
    "        \n",
    "# Samme men for datasæt 2\n",
    "vector_length = 100\n",
    "art_matrix2 = np.zeros((len(corpus2), vector_length))\n",
    "print(np.shape(art_matrix2))\n",
    "\n",
    "for i, text in enumerate(corpus2):\n",
    "        art_vect = np.mean([np.array(embeddings_index[w]) for w in text if w in embeddings_index.keys()],\n",
    "                           axis=0)\n",
    "        art_matrix2[i] = art_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c90e2c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8970    0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-a6ea3ce0193a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_matrix2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "x = np.argwhere(np.isnan(art_matrix))\n",
    "row = 0\n",
    "for i in x[0:]:\n",
    "    print(i)\n",
    "    if i > row:\n",
    "        print(row)\n",
    "z = np.argwhere(np.isnan(art_matrix2))\n",
    "print(z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9aa8c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training articles: 33673\n",
      "Test articles: 11225\n",
      "Second dataset\n",
      "Training articles: 15570\n",
      "Test articles: 5191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datasæt 1\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(art_matrix, y1, random_state=0)\n",
    "print(\"Training articles: {}\\nTest articles: {}\".format(np.size(y_train1), np.size(y_test1)))\n",
    "\n",
    "# Datasæt 2\n",
    "# Jeg gør det samme for datasæt 2, sådan så  jeg får 2 modeller\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(art_matrix2, y2, random_state=0)\n",
    "print(\"Second dataset\\nTraining articles: {}\\nTest articles: {}\".format(np.size(y_train2), np.size(y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c1a0c2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-09d46b9022b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset 1 training test score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset 1 test score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dataset 2 test score: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_matrix2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=1000).fit(X_train1, y_train1)\n",
    "print(\"Dataset 1 training test score: {}\".format(log_reg.score(X_train1, y_train1)))\n",
    "print(\"Dataset 1 test score: {}\".format(log_reg.score(X_test1, y_test1)))\n",
    "print(\"Dataset 2 test score: {}\".format(log_reg.score(art_matrix2, y2)))\n",
    "\n",
    "# Model trænet på det andet datasæt\n",
    "log_reg2 = LogisticRegression(max_iter=1000).fit(X_train2, y_train2)\n",
    "print(\"Second model dataset 1 test score: {}\".format(log_reg2.score(art_matrix1, y1)))\n",
    "print(\"First model dataset 2 test score: {}\".format(log_reg2.score(X_test2, y_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c78c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
