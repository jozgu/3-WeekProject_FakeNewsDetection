{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecba833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cd4311ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65698\n"
     ]
    }
   ],
   "source": [
    "# Importer det første datasæt\n",
    "df1 = pd.read_csv(\"True.csv\")\n",
    "df2 = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "X1 = np.array(df[\"text\"])\n",
    "\n",
    "# Lav target array, hvor 0 er en troværdig artikel og 1 er en utroværdig artikel\n",
    "a = np.zeros(len(df1))\n",
    "b = np.ones(len(df2))\n",
    "y1 = np.hstack((y1, y2))\n",
    "\n",
    "# Importer det andet datasæt\n",
    "df_second = pd.read_csv(\"Train.csv\")\n",
    "X2 = np.array(df_second[\"text\"])\n",
    "\n",
    "y2 = np.array(df_second[\"label\"])\n",
    "\n",
    "# Saml datasættene til et stort datasæt\n",
    "X = np.append(X1, X2)\n",
    "y = np.hstack((y1, y2))\n",
    "print(np.size(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "08efe0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399852 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Nu importerer jeg word_embeddingen\n",
    "# Det meste af denne kode er taget fra https://blog.paperspace.com/pre-trained-word-embeddings-natural-language-processing/\n",
    "# Jeg ønsker ikke at undersøge stopwords, så disse skal fjernes fra embeddings_index\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('stanford_embedding/glove.6B.100d.txt', 'r', encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    if word not in stop_words:\n",
    "        embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fb602d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No corrupted values\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word, v in embeddings_index.items():\n",
    "    if not isinstance(v, np.ndarray):\n",
    "        print(\"{} has an enmpty value\".format(word))\n",
    "        count += 1\n",
    "        \n",
    "if count == 0:\n",
    "    print(\"No corrupted values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f8a72923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65698,)\n",
      "(64539,)\n",
      "65698\n",
      "64539\n"
     ]
    }
   ],
   "source": [
    "# Dataen laves om, så den ikke indeholder specialkarakterer eller stopwords\n",
    "# Datasæt 1\n",
    "# Iterer over artiklerne og fjern specialtegn\n",
    "import re\n",
    "failed_files = []\n",
    "corpus = []\n",
    "for i, text in enumerate(X):\n",
    "    # try-statement benyttes, da nogen artikler blot indeholder nan, hvilket vil give fejl senere\n",
    "    try:\n",
    "        text_oa = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "        text_oa = text_oa.lower()\n",
    "        text_oa = text_oa.split()\n",
    "\n",
    "        # stopwords filtreres fra og artikler med 5 eller færrere ord tilbage fjernes, da vi ikke mener at de bidrager\n",
    "        # til opfattelsen af fake news. Dette hjælper også til at undgå fejl senere\n",
    "\n",
    "        filtered_article = [w for w in text_oa if w not in stop_words and len(w) > 1]\n",
    "        if len(filtered_article) > 5:\n",
    "            corpus.append(filtered_article)\n",
    "        else:\n",
    "            failed_files.append(i)\n",
    "    except TypeError:\n",
    "        failed_files.append(i)\n",
    "\n",
    "# Tilpas target, da nogen artikler er blevet fjernet\n",
    "print(np.shape(y))\n",
    "y = np.delete(y, failed_files)\n",
    "print(np.shape(y))\n",
    "\n",
    "print(len(X))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4facc863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64539, 100)\n"
     ]
    }
   ],
   "source": [
    "# Opret vektor-repræsentationer af dokumenter som vægtede gennemsnit af word-embedding\n",
    "# Datasæt 1\n",
    "# Initialiser matricen, der skal holde artikel-vektorerne\n",
    "vector_length = 100\n",
    "art_matrix = np.zeros((len(corpus), vector_length))\n",
    "print(np.shape(art_matrix))\n",
    "\n",
    "for i, text in enumerate(corpus):\n",
    "    list = [embeddings_index[w] for w in text if w in embeddings_index.keys()]\n",
    "    x = np.mean(list, axis=0)     \n",
    "    art_matrix[i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "10e52c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "x = np.argwhere(np.isnan(art_matrix))\n",
    "print(x)\n",
    "row = 0\n",
    "for i in x[:, 0]:\n",
    "    if i > row:\n",
    "        print(corpus1[i])\n",
    "        row = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "cd795f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training articles: 48404\n",
      "Test articles: 16135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Datasæt 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(art_matrix, y, random_state=0, shuffle=True)\n",
    "print(\"Training articles: {}\\nTest articles: {}\".format(np.size(y_train), np.size(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b792ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset training test score: 0.863957524171556\n",
      "Dataset test score: 0.8624728850325379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "print(\"Dataset training test score: {}\".format(log_reg.score(X_train, y_train)))\n",
    "print(\"Dataset test score: {}\".format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c47d9121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross-validation score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Jeg tester modellens generaliseringspotentiale med cross-validation på begge datasæt.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "first_scores = cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print(\"Average cross-validation score: {:.2f}\".format(first_scores.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c7415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
